
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Implementation of first assignment for CMSC828J</title><meta name="generator" content="MATLAB 7.12"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2013-03-03"><meta name="DC.source" content="Assignment1.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Implementation of first assignment for CMSC828J</h1><!--introduction--><p><b>Theme: LDA and its application in binary classification.</b></p><p>We use a subset of the "Arcene" dataset (<a href="http://archive.ics.uci.edu/ml/datasets/Arcene">http://archive.ics.uci.edu/ml/datasets/Arcene</a>) to compare the effect of PCA and LDA on projecting the data on a one-dimensional linear subspace, as well as measure the effect of LDA on binary classification through an SVM. Arcene contains mass-spectrometric data from cancer patients as well as cancer-free subjects. The dataset was part of the NIPS 2003 feature selection challenge, so the test data's labels are withheld. We therefore concatenate the training and validation data (200 examples total, with 88 positives and 112 negatives) to experiment.</p><p>One interesting element of this dataset is that of the 10000 features selected by the owners, 3000 were "probes", i.e uninformative features. This features were randomly permuted with the informative spectrometric data features, and the owners provided no information about the feature selection process whatsoever. Therefore, good feature selection algorithms would have to be able to discern between the informative and uninformative features for the classification task, which makes dimensionality reduction techniques a very attractive candidate for solving this problem.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Step 1: Read pre-processed data</a></li><li><a href="#2">Step 2: Apply PCA to the data</a></li><li><a href="#3">Step 3: Apply LDA to the projected data</a></li><li><a href="#4">Step 4: Visually compare the LDA projection with the projection along the top principal component.</a></li><li><a href="#5">Step 5 (final): Cross-validate a linear classifier on the original data and the LDA-projected data.</a></li><li><a href="#6">Conclusions:</a></li></ul></div><h2>Step 1: Read pre-processed data<a name="1"></a></h2><p>We have read all 200 examples and their corresponding labels into MATLAB variables, which we will now load into the workspace.</p><pre class="codeinput">clc;
load <span class="string">concatData</span>;
</pre><h2>Step 2: Apply PCA to the data<a name="2"></a></h2><p>As suggested by the "Fisherfaces" paper, in order to turn the "within class scatter" matrix <img src="Assignment1_eq29871.png" alt="$S_w$"> into a full rank matrix, the data needs to be reduced to <img src="Assignment1_eq63878.png" alt="$N - c = N - 1$"> dimensions. We therefore apply PCA to reduce the dimensionality. We will use MATLAB's PCA implementation for this, which carries out multiple optimizations. For instance, the 'econ' flag that we'll be using carries out the optimization mentioned in the "Fisherfaces" paper, by discarding all eigenvectors whose eigenvalues are zero. This will help us keep only the first <img src="Assignment1_eq19866.png" alt="$N - 1$"> eigenvectors, which is exactly what we want.</p><pre class="codeinput">eigv = princomp(all_data, <span class="string">'econ'</span>);
projected_data = eigv' * all_data';
projected_data = projected_data'; <span class="comment">% Now data is N x (N -1)</span>
</pre><h2>Step 3: Apply LDA to the projected data<a name="3"></a></h2><p>Now that the data has been compressed, we can call our LDA implementation without worrying about the "within class" scatter matrix. Refer to the function file LDA.m for details.</p><pre class="codeinput">positive_examples = projected_data(all_labels&gt;0, :);
negative_examples = projected_data(all_labels &lt;0, :);
w = LDA(positive_examples, negative_examples);
</pre><h2>Step 4: Visually compare the LDA projection with the projection along the top principal component.<a name="4"></a></h2><pre class="codeinput">princ_comp_proj = eigv(:, 1)' * all_data';
princ_comp_proj = princ_comp_proj';
pos_ex_princ_comp = princ_comp_proj(all_labels &gt; 0, :);
neg_ex_princ_comp = princ_comp_proj(all_labels &lt; 0, :);

<span class="comment">% PCA projection maximizes variance, but isn't helpful for classification.</span>

stem(pos_ex_princ_comp,<span class="string">'DisplayName'</span>,<span class="string">'Positive Examples'</span>,<span class="keyword">...</span>
    <span class="string">'Color'</span>, <span class="string">'b'</span>,<span class="keyword">...</span>
    <span class="string">'Marker'</span>, <span class="string">'.'</span>);
figure(gcf); hold <span class="string">on</span>;
stem(neg_ex_princ_comp,<span class="string">'DisplayName'</span>,<span class="string">'Negative Examples'</span>,<span class="keyword">...</span>
    <span class="string">'Color'</span>, <span class="string">'g'</span>,<span class="keyword">...</span>
    <span class="string">'Marker'</span>, <span class="string">'.'</span>);
figure(gcf);
legend(<span class="string">'Positive Examples'</span>, <span class="string">'Negative Examples'</span>,<span class="keyword">...</span>
    <span class="string">'Location'</span>, <span class="string">'SouthEast'</span>);
title(<span class="string">'Projection of classes along top principal component'</span>);
xlabel(<span class="string">'Example indices'</span>);
ylabel(<span class="string">'Example values'</span>);
hold <span class="string">off</span>;

<span class="comment">% On the other hand, LDA projection promotes class separability.</span>

pos_ex_LDA_proj = positive_examples * w;
neg_ex_LDA_proj = negative_examples * w;
figure;
stem(pos_ex_LDA_proj,<span class="string">'DisplayName'</span>,<span class="string">'Positive Examples'</span>,<span class="keyword">...</span>
    <span class="string">'Color'</span>, <span class="string">'b'</span>,<span class="keyword">...</span>
    <span class="string">'Marker'</span>, <span class="string">'.'</span>);
figure(gcf); hold <span class="string">on</span>;
stem(neg_ex_LDA_proj,<span class="string">'DisplayName'</span>,<span class="string">'Negative Examples'</span>,<span class="keyword">...</span>
    <span class="string">'Color'</span>, <span class="string">'g'</span>,<span class="keyword">...</span>
    <span class="string">'Marker'</span>, <span class="string">'.'</span>);
figure(gcf);
legend(<span class="string">'Positive Examples'</span>, <span class="string">'Negative Examples'</span>,<span class="keyword">...</span>
    <span class="string">'Location'</span>, <span class="string">'SouthEast'</span>);
title(<span class="string">'LDA projection of classes'</span>);
xlabel(<span class="string">'Example indices'</span>);
ylabel(<span class="string">'Example values'</span>);
hold <span class="string">off</span>;

<span class="comment">% It is not hard to see that LDA has separated the classes very well,</span>
<span class="comment">% whereas PCA has naturally "mangled them up" together to preserve the</span>
<span class="comment">% dataset's variance.</span>
</pre><img vspace="5" hspace="5" src="Assignment1_01.png" alt=""> <img vspace="5" hspace="5" src="Assignment1_02.png" alt=""> <h2>Step 5 (final): Cross-validate a linear classifier on the original data and the LDA-projected data.<a name="5"></a></h2><p>We will use MATLAB's SVM and cross-validation capabilities to estimate how well LDA might be suited with respect to linear classifiers.</p><pre class="codeinput"><span class="comment">% First we will need to concatenate the data with its labels, since this</span>
<span class="comment">% serves the purposes of the functions we will use.</span>
full_data = [all_data, all_labels];
full_LDA_data = [projected_data*w, all_labels];

<span class="comment">% Second, we will use the "crossval" function to run our SVM wrapper</span>
<span class="comment">% (implemented in SVM.m) over 10 different folds of both the original data</span>
<span class="comment">% and the LDA-projected data to obtain the relevant classification accuracies.</span>
<span class="comment">% We also store the CPU time it takes for both cross-validations to run.</span>
start = cputime;
orig_data_fold_accs = crossval(@SVM, full_data);
orig_data_fold_time = cputime - start;

start = cputime;
proj_data_fold_accs = crossval(@SVM, full_LDA_data);
proj_data_fold_time = cputime - start;

fprintf(<span class="string">'10-fold cross validation time on original Arcene data: %.4f seconds.\n'</span>, orig_data_fold_time);
fprintf(<span class="string">'Average accuracy: %.2f%%\n'</span>, 100.0*mean(orig_data_fold_accs));

fprintf(<span class="string">'10-fold cross validation time on LDA-projected Arcene data: %.4f seconds.\n'</span>, proj_data_fold_time);
fprintf(<span class="string">'Average accuracy: %.2f%%\n'</span>, 100.0*mean(proj_data_fold_accs));
</pre><pre class="codeoutput">10-fold cross validation time on original Arcene data: 3.8532 seconds.
Average accuracy: 88.50%
10-fold cross validation time on LDA-projected Arcene data: 0.2496 seconds.
Average accuracy: 92.50%
</pre><h2>Conclusions:<a name="6"></a></h2><p>We have proven the intuition behind LDA pictorially by comparing its result with the projection along the top principal component. We also showed that LDA can be very beneficial for linear  classifiers such as the SVM, by reducing the running time by orders of magnitude as well as benefit classification accuracy.</p><p class="footer"><br>
      Published with MATLAB&reg; 7.12<br></p></div><!--
##### SOURCE BEGIN #####
%% Implementation of first assignment for CMSC828J
% *Theme: LDA and its application in binary classification.*
%
% We use a subset of the "Arcene" dataset
% (http://archive.ics.uci.edu/ml/datasets/Arcene) to compare the effect
% of PCA and LDA on projecting the data on a one-dimensional linear
% subspace, as well as measure the effect of LDA on binary classification
% through an SVM. Arcene contains mass-spectrometric data from cancer 
% patients as well as cancer-free subjects. The dataset was part of the NIPS
% 2003 feature selection challenge, so the test data's labels are withheld.
% We therefore concatenate the training and validation data (200 examples 
% total, with 88 positives and 112 negatives) to experiment. 
%
% One interesting element of this dataset is that of the 10000 features
% selected by the owners, 3000 were "probes", i.e uninformative features.
% This features were randomly permuted with the informative spectrometric
% data features, and the owners provided no information about the feature 
% selection process whatsoever. Therefore, good feature selection algorithms 
% would have to be able to discern between the informative and uninformative 
% features for the classification task, which makes dimensionality reduction 
% techniques a very attractive candidate for solving this problem.


%% Step 1: Read pre-processed data
% We have read all 200 examples and their corresponding labels into 
% MATLAB variables, which we will now load into the workspace.

clc;
load concatData;

%% Step 2: Apply PCA to the data
% As suggested by the "Fisherfaces" paper, in order to turn the
% "within class scatter" matrix $S_w$ into a full rank matrix, the data needs to be
% reduced to $N - c = N - 1$ dimensions. We therefore apply PCA to reduce
% the dimensionality. We will use MATLAB's PCA implementation for this,
% which carries out multiple optimizations. For instance, the 'econ' flag
% that we'll be using carries out the optimization mentioned in the
% "Fisherfaces" paper, by discarding all eigenvectors whose eigenvalues are
% zero. This will help us keep only the first $N - 1$ eigenvectors, which
% is exactly what we want.

eigv = princomp(all_data, 'econ');
projected_data = eigv' * all_data';
projected_data = projected_data'; % Now data is N x (N -1)

%% Step 3: Apply LDA to the projected data
% Now that the data has been compressed, we can call our LDA
% implementation without worrying about the "within class" scatter matrix. 
% Refer to the function file LDA.m for details.


positive_examples = projected_data(all_labels>0, :);
negative_examples = projected_data(all_labels <0, :);
w = LDA(positive_examples, negative_examples);

%% Step 4: Visually compare the LDA projection with the projection along the top principal component. 

princ_comp_proj = eigv(:, 1)' * all_data';
princ_comp_proj = princ_comp_proj';
pos_ex_princ_comp = princ_comp_proj(all_labels > 0, :);
neg_ex_princ_comp = princ_comp_proj(all_labels < 0, :);

% PCA projection maximizes variance, but isn't helpful for classification.

stem(pos_ex_princ_comp,'DisplayName','Positive Examples',...
    'Color', 'b',...
    'Marker', '.');
figure(gcf); hold on;
stem(neg_ex_princ_comp,'DisplayName','Negative Examples',...
    'Color', 'g',...
    'Marker', '.');
figure(gcf); 
legend('Positive Examples', 'Negative Examples',...
    'Location', 'SouthEast');
title('Projection of classes along top principal component');
xlabel('Example indices');
ylabel('Example values');
hold off;

% On the other hand, LDA projection promotes class separability.

pos_ex_LDA_proj = positive_examples * w;
neg_ex_LDA_proj = negative_examples * w;
figure;
stem(pos_ex_LDA_proj,'DisplayName','Positive Examples',...
    'Color', 'b',...
    'Marker', '.');
figure(gcf); hold on;
stem(neg_ex_LDA_proj,'DisplayName','Negative Examples',...
    'Color', 'g',...
    'Marker', '.');
figure(gcf); 
legend('Positive Examples', 'Negative Examples',...
    'Location', 'SouthEast');
title('LDA projection of classes');
xlabel('Example indices');
ylabel('Example values');
hold off;

% It is not hard to see that LDA has separated the classes very well,
% whereas PCA has naturally "mangled them up" together to preserve the
% dataset's variance. 

%% Step 5 (final): Cross-validate a linear classifier on the original data and the LDA-projected data.
% We will use MATLAB's SVM and cross-validation capabilities to estimate
% how well LDA might be suited with respect to linear classifiers.

% First we will need to concatenate the data with its labels, since this
% serves the purposes of the functions we will use.
full_data = [all_data, all_labels];
full_LDA_data = [projected_data*w, all_labels];

% Second, we will use the "crossval" function to run our SVM wrapper
% (implemented in SVM.m) over 10 different folds of both the original data 
% and the LDA-projected data to obtain the relevant classification accuracies.
% We also store the CPU time it takes for both cross-validations to run.
start = cputime;
orig_data_fold_accs = crossval(@SVM, full_data);
orig_data_fold_time = cputime - start;

start = cputime;
proj_data_fold_accs = crossval(@SVM, full_LDA_data);
proj_data_fold_time = cputime - start;

fprintf('10-fold cross validation time on original Arcene data: %.4f seconds.\n', orig_data_fold_time);
fprintf('Average accuracy: %.2f%%\n', 100.0*mean(orig_data_fold_accs));

fprintf('10-fold cross validation time on LDA-projected Arcene data: %.4f seconds.\n', proj_data_fold_time);
fprintf('Average accuracy: %.2f%%\n', 100.0*mean(proj_data_fold_accs));

%% Conclusions:
% We have proven the intuition behind LDA pictorially by comparing its result
% with the projection along the top principal component. We also showed that
% LDA can be very beneficial for linear  classifiers such as the SVM, by 
% reducing the running time by orders of magnitude as well as benefit 
% classification accuracy. 
##### SOURCE END #####
--></body></html>